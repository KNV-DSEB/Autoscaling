{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 07: Autoscaling Simulation\n",
    "\n",
    "## Mục Tiêu\n",
    "- Mô phỏng autoscaling với các strategies khác nhau\n",
    "- So sánh Reactive vs Predictive scaling\n",
    "- Phân tích chi phí và hiệu suất\n",
    "- Tối ưu hóa scaling policies\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Thêm src vào path\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from src.data.preprocessor import load_timeseries, split_train_test\n",
    "from src.autoscaling.policy import ServerConfig, ScalingPolicy, AutoscalingEngine\n",
    "from src.autoscaling.simulator import AutoscalingSimulator, SimulationResult\n",
    "from src.autoscaling.cost_analyzer import CostAnalyzer, CostMetrics\n",
    "from src.models.lightgbm_forecaster import LightGBMForecaster\n",
    "from src.features.feature_engineering import TimeSeriesFeatureEngineer\n",
    "\n",
    "# Settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data và Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load time series\n",
    "df = load_timeseries('../data/processed/timeseries_15min.parquet')\n",
    "df_clean = df[df['is_storm_period'] == 0].copy()\n",
    "\n",
    "# Use test data for simulation\n",
    "train, test = split_train_test(df_clean, test_start='1995-08-23')\n",
    "\n",
    "print(f\"Simulation period: {test.index.min()} to {test.index.max()}\")\n",
    "print(f\"Simulation length: {len(test)} intervals ({len(test)*15/60:.1f} hours)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model for predictions\n",
    "try:\n",
    "    model = LightGBMForecaster.load('../models/lightgbm_15min.pkl')\n",
    "    print(\"LightGBM model loaded!\")\n",
    "    \n",
    "    # Prepare features for predictions\n",
    "    fe = TimeSeriesFeatureEngineer(df_clean)\n",
    "    df_features = fe.create_all_features(target_col='request_count', granularity='15min')\n",
    "    feature_cols = fe.get_feature_columns(df_features)\n",
    "    X, y = fe.prepare_supervised(df_features, 'request_count', feature_cols, forecast_horizon=1)\n",
    "    \n",
    "    test_mask = X.index >= '1995-08-23'\n",
    "    X_test = X[test_mask]\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions = model.predict(X_test)\n",
    "    predicted_demand = pd.Series(predictions, index=X_test.index)\n",
    "    print(f\"Predictions generated: {len(predicted_demand)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not load model: {e}\")\n",
    "    print(\"Using simple moving average for predictions...\")\n",
    "    predicted_demand = test['request_count'].rolling(4).mean().shift(1).fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server Configuration\n",
    "server_config = ServerConfig(\n",
    "    max_requests_per_min=1000,      # 1000 requests/min per server\n",
    "    max_bytes_per_min=50_000_000,   # 50MB/min per server\n",
    "    min_servers=1,\n",
    "    max_servers=50,\n",
    "    cost_per_server_hour=0.10       # $0.10/hour per server\n",
    ")\n",
    "\n",
    "print(\"Server Configuration:\")\n",
    "print(f\"  Max Requests/min: {server_config.max_requests_per_min:,}\")\n",
    "print(f\"  Min Servers: {server_config.min_servers}\")\n",
    "print(f\"  Max Servers: {server_config.max_servers}\")\n",
    "print(f\"  Cost: ${server_config.cost_per_server_hour}/hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling Policy\n",
    "scaling_policy = ScalingPolicy(\n",
    "    scale_out_threshold=0.8,      # Scale out at 80% utilization\n",
    "    scale_in_threshold=0.3,       # Scale in at 30% utilization  \n",
    "    cooldown_period=5,            # 5 intervals (75 min) cooldown\n",
    "    consecutive_breaches=3,       # Need 3 consecutive breaches\n",
    "    scale_out_increment=2,        # Add 2 servers\n",
    "    scale_in_decrement=1,         # Remove 1 server\n",
    "    prediction_horizon=4,         # Look 1 hour ahead (4 * 15min)\n",
    "    predictive_buffer=0.2         # 20% safety buffer\n",
    ")\n",
    "\n",
    "print(\"\\nScaling Policy:\")\n",
    "print(f\"  Scale Out Threshold: {scaling_policy.scale_out_threshold*100}%\")\n",
    "print(f\"  Scale In Threshold: {scaling_policy.scale_in_threshold*100}%\")\n",
    "print(f\"  Cooldown: {scaling_policy.cooldown_period} intervals\")\n",
    "print(f\"  Consecutive Breaches: {scaling_policy.consecutive_breaches}\")\n",
    "print(f\"  Predictive Buffer: {scaling_policy.predictive_buffer*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Simulator\n",
    "simulator = AutoscalingSimulator(\n",
    "    server_config=server_config,\n",
    "    scaling_policy=scaling_policy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare actual demand series\n",
    "actual_demand = test['request_count'].copy()\n",
    "print(f\"Demand statistics:\")\n",
    "print(f\"  Mean: {actual_demand.mean():.0f} requests/interval\")\n",
    "print(f\"  Max: {actual_demand.max():.0f} requests/interval\")\n",
    "print(f\"  Min: {actual_demand.min():.0f} requests/interval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Reactive Scaling (Based on Current Load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reactive scaling simulation\n",
    "print(\"Running Reactive Scaling Simulation...\")\n",
    "reactive_result = simulator.run_simulation(\n",
    "    actual_demand=actual_demand,\n",
    "    predicted_demand=None,  # No predictions = reactive only\n",
    "    initial_servers=5,\n",
    "    freq_minutes=15\n",
    ")\n",
    "\n",
    "print(f\"\\nReactive Scaling Results:\")\n",
    "print(f\"  Total Cost: ${reactive_result.total_cost:.2f}\")\n",
    "print(f\"  Avg Servers: {reactive_result.average_servers:.1f}\")\n",
    "print(f\"  Scaling Events: {reactive_result.scaling_events}\")\n",
    "print(f\"  SLA Violations: {reactive_result.sla_violations}\")\n",
    "print(f\"  Dropped Requests: {reactive_result.dropped_requests:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Predictive Scaling (Based on Forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictive scaling simulation\n",
    "print(\"Running Predictive Scaling Simulation...\")\n",
    "\n",
    "# Align predictions with actual demand\n",
    "pred_aligned = predicted_demand.reindex(actual_demand.index).fillna(method='ffill')\n",
    "\n",
    "predictive_result = simulator.run_simulation(\n",
    "    actual_demand=actual_demand,\n",
    "    predicted_demand=pred_aligned,\n",
    "    initial_servers=5,\n",
    "    freq_minutes=15\n",
    ")\n",
    "\n",
    "print(f\"\\nPredictive Scaling Results:\")\n",
    "print(f\"  Total Cost: ${predictive_result.total_cost:.2f}\")\n",
    "print(f\"  Avg Servers: {predictive_result.average_servers:.1f}\")\n",
    "print(f\"  Scaling Events: {predictive_result.scaling_events}\")\n",
    "print(f\"  SLA Violations: {predictive_result.sla_violations}\")\n",
    "print(f\"  Dropped Requests: {predictive_result.dropped_requests:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Strategy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all strategies\n",
    "strategies = {\n",
    "    'Reactive': reactive_result,\n",
    "    'Predictive': predictive_result\n",
    "}\n",
    "\n",
    "comparison_df = simulator.compare_strategies(\n",
    "    actual_demand=actual_demand,\n",
    "    predicted_demand=pred_aligned,\n",
    "    initial_servers=5,\n",
    "    freq_minutes=15\n",
    ")\n",
    "\n",
    "print(\"\\nStrategy Comparison:\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Cost comparison\n",
    "costs = [reactive_result.total_cost, predictive_result.total_cost]\n",
    "axes[0, 0].bar(['Reactive', 'Predictive'], costs, color=['coral', 'steelblue'])\n",
    "axes[0, 0].set_title('Total Cost ($)')\n",
    "axes[0, 0].set_ylabel('Cost ($)')\n",
    "for i, v in enumerate(costs):\n",
    "    axes[0, 0].text(i, v + 0.5, f'${v:.2f}', ha='center')\n",
    "\n",
    "# Average servers\n",
    "avg_servers = [reactive_result.average_servers, predictive_result.average_servers]\n",
    "axes[0, 1].bar(['Reactive', 'Predictive'], avg_servers, color=['coral', 'steelblue'])\n",
    "axes[0, 1].set_title('Average Servers')\n",
    "axes[0, 1].set_ylabel('Servers')\n",
    "for i, v in enumerate(avg_servers):\n",
    "    axes[0, 1].text(i, v + 0.1, f'{v:.1f}', ha='center')\n",
    "\n",
    "# Scaling events\n",
    "events = [reactive_result.scaling_events, predictive_result.scaling_events]\n",
    "axes[1, 0].bar(['Reactive', 'Predictive'], events, color=['coral', 'steelblue'])\n",
    "axes[1, 0].set_title('Scaling Events')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "for i, v in enumerate(events):\n",
    "    axes[1, 0].text(i, v + 0.5, str(v), ha='center')\n",
    "\n",
    "# SLA violations\n",
    "violations = [reactive_result.sla_violations, predictive_result.sla_violations]\n",
    "axes[1, 1].bar(['Reactive', 'Predictive'], violations, color=['coral', 'steelblue'])\n",
    "axes[1, 1].set_title('SLA Violations')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "for i, v in enumerate(violations):\n",
    "    axes[1, 1].text(i, v + 0.5, str(v), ha='center')\n",
    "\n",
    "plt.suptitle('Reactive vs Predictive Scaling', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/autoscaling_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Timeline Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot server allocation over time\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Demand\n",
    "axes[0].plot(actual_demand.index, actual_demand.values, color='black', label='Actual Demand')\n",
    "axes[0].plot(pred_aligned.index, pred_aligned.values, color='green', alpha=0.5, \n",
    "             linestyle='--', label='Predicted Demand')\n",
    "axes[0].set_ylabel('Requests')\n",
    "axes[0].set_title('Traffic Demand')\n",
    "axes[0].legend()\n",
    "\n",
    "# Reactive servers\n",
    "reactive_servers = reactive_result.timeline['servers']\n",
    "axes[1].plot(actual_demand.index[:len(reactive_servers)], reactive_servers, \n",
    "             color='coral', label='Reactive', linewidth=2)\n",
    "axes[1].set_ylabel('Servers')\n",
    "axes[1].set_title('Server Allocation - Reactive')\n",
    "axes[1].legend()\n",
    "\n",
    "# Predictive servers\n",
    "predictive_servers = predictive_result.timeline['servers']\n",
    "axes[2].plot(actual_demand.index[:len(predictive_servers)], predictive_servers, \n",
    "             color='steelblue', label='Predictive', linewidth=2)\n",
    "axes[2].set_ylabel('Servers')\n",
    "axes[2].set_title('Server Allocation - Predictive')\n",
    "axes[2].set_xlabel('Timestamp')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/autoscaling_timeline.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilization comparison\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "reactive_util = reactive_result.timeline['utilization']\n",
    "predictive_util = predictive_result.timeline['utilization']\n",
    "\n",
    "axes[0].plot(actual_demand.index[:len(reactive_util)], reactive_util, color='coral')\n",
    "axes[0].axhline(y=scaling_policy.scale_out_threshold, color='red', linestyle='--', \n",
    "                label=f'Scale Out ({scaling_policy.scale_out_threshold*100}%)')\n",
    "axes[0].axhline(y=scaling_policy.scale_in_threshold, color='green', linestyle='--',\n",
    "                label=f'Scale In ({scaling_policy.scale_in_threshold*100}%)')\n",
    "axes[0].fill_between(actual_demand.index[:len(reactive_util)], 0, reactive_util, alpha=0.3, color='coral')\n",
    "axes[0].set_ylabel('Utilization')\n",
    "axes[0].set_title('Server Utilization - Reactive')\n",
    "axes[0].legend(loc='upper right')\n",
    "axes[0].set_ylim(0, 1.2)\n",
    "\n",
    "axes[1].plot(actual_demand.index[:len(predictive_util)], predictive_util, color='steelblue')\n",
    "axes[1].axhline(y=scaling_policy.scale_out_threshold, color='red', linestyle='--')\n",
    "axes[1].axhline(y=scaling_policy.scale_in_threshold, color='green', linestyle='--')\n",
    "axes[1].fill_between(actual_demand.index[:len(predictive_util)], 0, predictive_util, alpha=0.3, color='steelblue')\n",
    "axes[1].set_ylabel('Utilization')\n",
    "axes[1].set_title('Server Utilization - Predictive')\n",
    "axes[1].set_xlabel('Timestamp')\n",
    "axes[1].set_ylim(0, 1.2)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/autoscaling_utilization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost analyzer\n",
    "cost_analyzer = CostAnalyzer(server_config)\n",
    "\n",
    "# Generate cost report\n",
    "reactive_servers_series = pd.Series(\n",
    "    reactive_result.timeline['servers'], \n",
    "    index=actual_demand.index[:len(reactive_result.timeline['servers'])]\n",
    ")\n",
    "\n",
    "report = cost_analyzer.generate_cost_report(\n",
    "    demand_series=actual_demand[:len(reactive_servers_series)],\n",
    "    server_series=reactive_servers_series,\n",
    "    freq_minutes=15\n",
    ")\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost comparison with fixed provisioning\n",
    "demand_aligned = actual_demand[:len(predictive_servers)]\n",
    "\n",
    "# Calculate costs for different strategies\n",
    "fixed_peak, fixed_peak_metrics = cost_analyzer.calculate_optimal_fixed_servers(\n",
    "    demand_aligned, sla_target=99.0, freq_minutes=15\n",
    ")\n",
    "\n",
    "print(f\"\\nOptimal Fixed Servers for 99% SLA: {fixed_peak}\")\n",
    "print(f\"Fixed Cost: ${fixed_peak_metrics.total_cost:.2f}\")\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  Fixed Provisioning: ${fixed_peak_metrics.total_cost:.2f}\")\n",
    "print(f\"  Reactive Scaling: ${reactive_result.total_cost:.2f}\")\n",
    "print(f\"  Predictive Scaling: ${predictive_result.total_cost:.2f}\")\n",
    "\n",
    "savings_reactive = fixed_peak_metrics.total_cost - reactive_result.total_cost\n",
    "savings_predictive = fixed_peak_metrics.total_cost - predictive_result.total_cost\n",
    "\n",
    "print(f\"\\nSavings vs Fixed:\")\n",
    "print(f\"  Reactive: ${savings_reactive:.2f} ({savings_reactive/fixed_peak_metrics.total_cost*100:.1f}%)\")\n",
    "print(f\"  Predictive: ${savings_predictive:.2f} ({savings_predictive/fixed_peak_metrics.total_cost*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different buffer sizes\n",
    "print(\"Running sensitivity analysis on predictive buffer...\")\n",
    "\n",
    "buffer_results = simulator.run_sensitivity_analysis(\n",
    "    actual_demand=actual_demand,\n",
    "    predicted_demand=pred_aligned,\n",
    "    parameter='predictive_buffer',\n",
    "    values=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    initial_servers=5,\n",
    "    freq_minutes=15\n",
    ")\n",
    "\n",
    "print(buffer_results[['Parameter Value', 'Total Cost', 'Avg Servers', \n",
    "                      'SLA Violations', 'Dropped Requests']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sensitivity analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "buffer_values = buffer_results['Parameter Value'].values\n",
    "\n",
    "# Cost vs Buffer\n",
    "axes[0].plot(buffer_values, buffer_results['Total Cost'], marker='o', color='steelblue')\n",
    "axes[0].set_xlabel('Predictive Buffer')\n",
    "axes[0].set_ylabel('Total Cost ($)')\n",
    "axes[0].set_title('Cost vs Buffer Size')\n",
    "\n",
    "# SLA Violations vs Buffer\n",
    "axes[1].plot(buffer_values, buffer_results['SLA Violations'], marker='o', color='coral')\n",
    "axes[1].set_xlabel('Predictive Buffer')\n",
    "axes[1].set_ylabel('SLA Violations')\n",
    "axes[1].set_title('SLA Violations vs Buffer Size')\n",
    "\n",
    "# Avg Servers vs Buffer\n",
    "axes[2].plot(buffer_values, buffer_results['Avg Servers'], marker='o', color='seagreen')\n",
    "axes[2].set_xlabel('Predictive Buffer')\n",
    "axes[2].set_ylabel('Average Servers')\n",
    "axes[2].set_title('Avg Servers vs Buffer Size')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/autoscaling_sensitivity.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"              AUTOSCALING SIMULATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nSimulation Period: {test.index.min()} to {test.index.max()}\")\n",
    "print(f\"Duration: {len(test)*15/60:.1f} hours ({len(test)} intervals)\")\n",
    "print(f\"\\nServer Configuration:\")\n",
    "print(f\"  Capacity: {server_config.max_requests_per_min} requests/min\")\n",
    "print(f\"  Cost: ${server_config.cost_per_server_hour}/hour\")\n",
    "print(f\"\\n\" + \"-\"*70)\n",
    "print(\"STRATEGY COMPARISON:\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Strategy':<15} {'Cost ($)':<12} {'Avg Servers':<12} {'SLA Violations':<15} {'Dropped Req':<12}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Fixed Peak':<15} {fixed_peak_metrics.total_cost:<12.2f} {fixed_peak:<12} {fixed_peak_metrics.underprovisioning_events:<15} {fixed_peak_metrics.dropped_requests:<12,}\")\n",
    "print(f\"{'Reactive':<15} {reactive_result.total_cost:<12.2f} {reactive_result.average_servers:<12.1f} {reactive_result.sla_violations:<15} {reactive_result.dropped_requests:<12,}\")\n",
    "print(f\"{'Predictive':<15} {predictive_result.total_cost:<12.2f} {predictive_result.average_servers:<12.1f} {predictive_result.sla_violations:<15} {predictive_result.dropped_requests:<12,}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"\\nKEY FINDINGS:\")\n",
    "print(f\"  - Predictive scaling saves ${savings_predictive:.2f} vs fixed provisioning\")\n",
    "print(f\"  - Cost reduction: {savings_predictive/fixed_peak_metrics.total_cost*100:.1f}%\")\n",
    "best_buffer = buffer_results.loc[buffer_results['SLA Violations'].idxmin(), 'Parameter Value']\n",
    "print(f\"  - Optimal buffer: {best_buffer*100:.0f}% (minimizes SLA violations)\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
