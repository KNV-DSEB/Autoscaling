{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 07: Autoscaling Simulation\n",
    "\n",
    "## M\u1ee5c Ti\u00eau\n",
    "- M\u00f4 ph\u1ecfng autoscaling v\u1edbi c\u00e1c strategies kh\u00e1c nhau\n",
    "- So s\u00e1nh Reactive vs Predictive scaling\n",
    "- Ph\u00e2n t\u00edch chi ph\u00ed v\u00e0 hi\u1ec7u su\u1ea5t\n",
    "- T\u1ed1i \u01b0u h\u00f3a scaling policies\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sys\nimport os\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n# Th\u00eam src v\u00e0o path\nsys.path.insert(0, os.path.abspath('..'))\n\nfrom src.data.preprocessor import load_timeseries, split_train_test\nfrom src.autoscaling.policy import ServerConfig, ScalingPolicy, AutoscalingEngine\nfrom src.autoscaling.simulator import AutoscalingSimulator\nfrom src.autoscaling.cost_analyzer import CostAnalyzer\nfrom src.models.lightgbm_forecaster import LightGBMForecaster\nfrom src.features.feature_engineering import TimeSeriesFeatureEngineer\n\n# Settings\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams['figure.figsize'] = (14, 6)\n\nprint(\"Libraries loaded successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data v\u00e0 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load time series\n",
    "df = load_timeseries('../data/processed/timeseries_15min.parquet')\n",
    "df_clean = df[df['is_storm_period'] == 0].copy()\n",
    "\n",
    "# Use test data for simulation\n",
    "train, test = split_train_test(df_clean, test_start='1995-08-23')\n",
    "\n",
    "print(f\"Simulation period: {test.index.min()} to {test.index.max()}\")\n",
    "print(f\"Simulation length: {len(test)} intervals ({len(test)*15/60:.1f} hours)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load trained model for predictions\ntry:\n    model = LightGBMForecaster.load('../models/lightgbm_15min.pkl')\n    print(\"LightGBM model loaded!\")\n    \n    # Prepare features for predictions\n    fe = TimeSeriesFeatureEngineer(df_clean)\n    df_features = fe.create_all_features(target_col='request_count', granularity='15min')\n    feature_cols = fe.get_feature_columns(df_features)\n    X, y = fe.prepare_supervised(df_features, 'request_count', feature_cols, forecast_horizon=1)\n    \n    test_mask = X.index >= '1995-08-23'\n    X_test = X[test_mask]\n    \n    # Generate predictions\n    predictions = model.predict(X_test)\n    predicted_demand = pd.Series(predictions, index=X_test.index)\n    print(f\"Predictions generated: {len(predicted_demand)}\")\n    \nexcept Exception as e:\n    print(f\"Could not load model: {e}\")\n    print(\"Using simple moving average for predictions...\")\n    predicted_demand = test['request_count'].rolling(4).mean().shift(1).bfill()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server Configuration\n",
    "server_config = ServerConfig(\n",
    "    max_requests_per_min=1000,      # 1000 requests/min per server\n",
    "    max_bytes_per_min=50_000_000,   # 50MB/min per server\n",
    "    min_servers=1,\n",
    "    max_servers=50,\n",
    "    cost_per_server_hour=0.10       # $0.10/hour per server\n",
    ")\n",
    "\n",
    "print(\"Server Configuration:\")\n",
    "print(f\"  Max Requests/min: {server_config.max_requests_per_min:,}\")\n",
    "print(f\"  Min Servers: {server_config.min_servers}\")\n",
    "print(f\"  Max Servers: {server_config.max_servers}\")\n",
    "print(f\"  Cost: ${server_config.cost_per_server_hour}/hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling Policy\n",
    "scaling_policy = ScalingPolicy(\n",
    "    scale_out_threshold=0.8,      # Scale out at 80% utilization\n",
    "    scale_in_threshold=0.3,       # Scale in at 30% utilization  \n",
    "    cooldown_period=5,            # 5 intervals (75 min) cooldown\n",
    "    consecutive_breaches=3,       # Need 3 consecutive breaches\n",
    "    scale_out_increment=2,        # Add 2 servers\n",
    "    scale_in_decrement=1,         # Remove 1 server\n",
    "    prediction_horizon=4,         # Look 1 hour ahead (4 * 15min)\n",
    "    predictive_buffer=0.2         # 20% safety buffer\n",
    ")\n",
    "\n",
    "print(\"\\nScaling Policy:\")\n",
    "print(f\"  Scale Out Threshold: {scaling_policy.scale_out_threshold*100}%\")\n",
    "print(f\"  Scale In Threshold: {scaling_policy.scale_in_threshold*100}%\")\n",
    "print(f\"  Cooldown: {scaling_policy.cooldown_period} intervals\")\n",
    "print(f\"  Consecutive Breaches: {scaling_policy.consecutive_breaches}\")\n",
    "print(f\"  Predictive Buffer: {scaling_policy.predictive_buffer*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize Simulator\nsimulator = AutoscalingSimulator(\n    server_config=server_config,\n    policy=scaling_policy\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare actual demand series\n",
    "actual_demand = test['request_count'].copy()\n",
    "print(f\"Demand statistics:\")\n",
    "print(f\"  Mean: {actual_demand.mean():.0f} requests/interval\")\n",
    "print(f\"  Max: {actual_demand.max():.0f} requests/interval\")\n",
    "print(f\"  Min: {actual_demand.min():.0f} requests/interval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Reactive Scaling (Based on Current Load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Reactive scaling simulation\nprint(\"Running Reactive Scaling Simulation...\")\nreactive_df = simulator.simulate(\n    actual_data=test,\n    predictions=None,  # No predictions = reactive only\n    use_predictive=False,\n    initial_servers=5\n)\n\nreactive_metrics = simulator.calculate_metrics(reactive_df)\n\nprint(f\"\\nReactive Scaling Results:\")\nprint(f\"  Total Cost: ${reactive_metrics['total_cost']:.2f}\")\nprint(f\"  Avg Servers: {reactive_metrics['avg_servers']:.1f}\")\nprint(f\"  Scaling Events: {reactive_metrics['total_scaling_events']}\")\nprint(f\"  SLA Violations: {reactive_metrics['overloaded_periods']}\")\nprint(f\"  Dropped Requests: {reactive_metrics['total_dropped_requests']:,.0f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Predictive Scaling (Based on Forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Predictive scaling simulation\nprint(\"Running Predictive Scaling Simulation...\")\n\n# Align predictions with actual demand - create predictions DataFrame\npred_aligned = predicted_demand.reindex(test.index).ffill()\n\n# Build predictions DataFrame with request_count column\npred_df = test.copy()\npred_df['request_count'] = pred_aligned.values\n\npredictive_df = simulator.simulate(\n    actual_data=test,\n    predictions=pred_df,\n    use_predictive=True,\n    initial_servers=5\n)\n\npredictive_metrics = simulator.calculate_metrics(predictive_df)\n\nprint(f\"\\nPredictive Scaling Results:\")\nprint(f\"  Total Cost: ${predictive_metrics['total_cost']:.2f}\")\nprint(f\"  Avg Servers: {predictive_metrics['avg_servers']:.1f}\")\nprint(f\"  Scaling Events: {predictive_metrics['total_scaling_events']}\")\nprint(f\"  SLA Violations: {predictive_metrics['overloaded_periods']}\")\nprint(f\"  Dropped Requests: {predictive_metrics['total_dropped_requests']:,.0f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Strategy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare all strategies using simulator's built-in comparison\ncomparison_df = simulator.compare_strategies(\n    actual_data=test,\n    predictions=pred_df\n)\n\nprint(\"\\nStrategy Comparison:\")\nprint(comparison_df.to_string(index=False))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visual comparison\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Cost comparison\ncosts = [reactive_metrics['total_cost'], predictive_metrics['total_cost']]\naxes[0, 0].bar(['Reactive', 'Predictive'], costs, color=['coral', 'steelblue'])\naxes[0, 0].set_title('Total Cost ($)')\naxes[0, 0].set_ylabel('Cost ($)')\nfor i, v in enumerate(costs):\n    axes[0, 0].text(i, v + 0.5, f'${v:.2f}', ha='center')\n\n# Average servers\navg_servers = [reactive_metrics['avg_servers'], predictive_metrics['avg_servers']]\naxes[0, 1].bar(['Reactive', 'Predictive'], avg_servers, color=['coral', 'steelblue'])\naxes[0, 1].set_title('Average Servers')\naxes[0, 1].set_ylabel('Servers')\nfor i, v in enumerate(avg_servers):\n    axes[0, 1].text(i, v + 0.1, f'{v:.1f}', ha='center')\n\n# Scaling events\nevents = [reactive_metrics['total_scaling_events'], predictive_metrics['total_scaling_events']]\naxes[1, 0].bar(['Reactive', 'Predictive'], events, color=['coral', 'steelblue'])\naxes[1, 0].set_title('Scaling Events')\naxes[1, 0].set_ylabel('Count')\nfor i, v in enumerate(events):\n    axes[1, 0].text(i, v + 0.5, str(v), ha='center')\n\n# SLA violations\nviolations = [reactive_metrics['overloaded_periods'], predictive_metrics['overloaded_periods']]\naxes[1, 1].bar(['Reactive', 'Predictive'], violations, color=['coral', 'steelblue'])\naxes[1, 1].set_title('SLA Violations')\naxes[1, 1].set_ylabel('Count')\nfor i, v in enumerate(violations):\n    axes[1, 1].text(i, v + 0.5, str(v), ha='center')\n\nplt.suptitle('Reactive vs Predictive Scaling', fontsize=14)\nplt.tight_layout()\nplt.savefig('../reports/figures/autoscaling_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Timeline Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot server allocation over time\nfig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n\nactual_demand = test['request_count']\n\n# Demand\naxes[0].plot(actual_demand.index, actual_demand.values, color='black', label='Actual Demand')\npred_aligned_plot = predicted_demand.reindex(actual_demand.index).ffill()\naxes[0].plot(pred_aligned_plot.index, pred_aligned_plot.values, color='green', alpha=0.5, \n             linestyle='--', label='Predicted Demand')\naxes[0].set_ylabel('Requests')\naxes[0].set_title('Traffic Demand')\naxes[0].legend()\n\n# Reactive servers\naxes[1].plot(reactive_df.index, reactive_df['servers'].values, \n             color='coral', label='Reactive', linewidth=2)\naxes[1].set_ylabel('Servers')\naxes[1].set_title('Server Allocation - Reactive')\naxes[1].legend()\n\n# Predictive servers\naxes[2].plot(predictive_df.index, predictive_df['servers'].values, \n             color='steelblue', label='Predictive', linewidth=2)\naxes[2].set_ylabel('Servers')\naxes[2].set_title('Server Allocation - Predictive')\naxes[2].set_xlabel('Timestamp')\naxes[2].legend()\n\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.savefig('../reports/figures/autoscaling_timeline.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Utilization comparison\nfig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n\nreactive_util = reactive_df['utilization']\npredictive_util = predictive_df['utilization']\n\naxes[0].plot(reactive_df.index, reactive_util.values, color='coral')\naxes[0].axhline(y=scaling_policy.scale_out_threshold, color='red', linestyle='--', \n                label=f'Scale Out ({scaling_policy.scale_out_threshold*100}%)')\naxes[0].axhline(y=scaling_policy.scale_in_threshold, color='green', linestyle='--',\n                label=f'Scale In ({scaling_policy.scale_in_threshold*100}%)')\naxes[0].fill_between(reactive_df.index, 0, reactive_util.values, alpha=0.3, color='coral')\naxes[0].set_ylabel('Utilization')\naxes[0].set_title('Server Utilization - Reactive')\naxes[0].legend(loc='upper right')\naxes[0].set_ylim(0, 1.2)\n\naxes[1].plot(predictive_df.index, predictive_util.values, color='steelblue')\naxes[1].axhline(y=scaling_policy.scale_out_threshold, color='red', linestyle='--')\naxes[1].axhline(y=scaling_policy.scale_in_threshold, color='green', linestyle='--')\naxes[1].fill_between(predictive_df.index, 0, predictive_util.values, alpha=0.3, color='steelblue')\naxes[1].set_ylabel('Utilization')\naxes[1].set_title('Server Utilization - Predictive')\naxes[1].set_xlabel('Timestamp')\naxes[1].set_ylim(0, 1.2)\n\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.savefig('../reports/figures/autoscaling_utilization.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cost analyzer\ncost_analyzer = CostAnalyzer(server_config)\n\n# Generate cost report using reactive simulation results\nreactive_servers_series = reactive_df['servers']\nactual_demand = test['request_count']\n\nreport = cost_analyzer.generate_cost_report(\n    demand_series=actual_demand[:len(reactive_servers_series)],\n    server_series=reactive_servers_series,\n    freq_minutes=15\n)\n\nprint(report)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cost comparison with fixed provisioning\ndemand_aligned = actual_demand[:len(predictive_df)]\n\n# Calculate costs for different strategies\nfixed_peak, fixed_peak_metrics = cost_analyzer.calculate_optimal_fixed_servers(\n    demand_aligned, sla_target=99.0, freq_minutes=15\n)\n\nprint(f\"\\nOptimal Fixed Servers for 99% SLA: {fixed_peak}\")\nprint(f\"Fixed Cost: ${fixed_peak_metrics.total_cost:.2f}\")\nprint(f\"\\nComparison:\")\nprint(f\"  Fixed Provisioning: ${fixed_peak_metrics.total_cost:.2f}\")\nprint(f\"  Reactive Scaling: ${reactive_metrics['total_cost']:.2f}\")\nprint(f\"  Predictive Scaling: ${predictive_metrics['total_cost']:.2f}\")\n\nsavings_reactive = fixed_peak_metrics.total_cost - reactive_metrics['total_cost']\nsavings_predictive = fixed_peak_metrics.total_cost - predictive_metrics['total_cost']\n\nprint(f\"\\nSavings vs Fixed:\")\nprint(f\"  Reactive: ${savings_reactive:.2f} ({savings_reactive/fixed_peak_metrics.total_cost*100:.1f}%)\")\nprint(f\"  Predictive: ${savings_predictive:.2f} ({savings_predictive/fixed_peak_metrics.total_cost*100:.1f}%)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test different buffer sizes\nprint(\"Running sensitivity analysis on predictive buffer...\")\n\nbuffer_results = simulator.run_sensitivity_analysis(\n    actual_data=test,\n    predictions=pred_df,  # Use DataFrame not Series\n    param_name='predictive_buffer',\n    param_values=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n)\n\n# Show results with correct column names\nprint(buffer_results[['predictive_buffer', 'total_cost', 'avg_servers', \n                      'overloaded_periods', 'total_dropped_requests']].to_string(index=False))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize sensitivity analysis\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\nbuffer_values = buffer_results['predictive_buffer'].values\n\n# Cost vs Buffer\naxes[0].plot(buffer_values, buffer_results['total_cost'], marker='o', color='steelblue')\naxes[0].set_xlabel('Predictive Buffer')\naxes[0].set_ylabel('Total Cost ($)')\naxes[0].set_title('Cost vs Buffer Size')\n\n# SLA Violations vs Buffer\naxes[1].plot(buffer_values, buffer_results['overloaded_periods'], marker='o', color='coral')\naxes[1].set_xlabel('Predictive Buffer')\naxes[1].set_ylabel('overloaded_periods')\naxes[1].set_title('SLA Violations vs Buffer Size')\n\n# Avg Servers vs Buffer\naxes[2].plot(buffer_values, buffer_results['avg_servers'], marker='o', color='seagreen')\naxes[2].set_xlabel('Predictive Buffer')\naxes[2].set_ylabel('Average Servers')\naxes[2].set_title('Avg Servers vs Buffer Size')\n\nplt.tight_layout()\nplt.savefig('../reports/figures/autoscaling_sensitivity.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*70)\nprint(\"              AUTOSCALING SIMULATION SUMMARY\")\nprint(\"=\"*70)\nprint(f\"\\nSimulation Period: {test.index.min()} to {test.index.max()}\")\nprint(f\"Duration: {len(test)*15/60:.1f} hours ({len(test)} intervals)\")\nprint(f\"\\nServer Configuration:\")\nprint(f\"  Capacity: {server_config.max_requests_per_min} requests/min\")\nprint(f\"  Cost: ${server_config.cost_per_server_hour}/hour\")\nprint(f\"\\n\" + \"-\"*70)\nprint(\"STRATEGY COMPARISON:\")\nprint(\"-\"*70)\nprint(f\"{'Strategy':<15} {'Cost ($)':<12} {'Avg Servers':<12} {'SLA Violations':<15} {'Dropped Req':<12}\")\nprint(\"-\"*70)\nprint(f\"{'Fixed Peak':<15} {fixed_peak_metrics.total_cost:<12.2f} {fixed_peak:<12} {fixed_peak_metrics.underprovisioning_events:<15} {fixed_peak_metrics.dropped_requests:<12,}\")\nprint(f\"{'Reactive':<15} {reactive_metrics['total_cost']:<12.2f} {reactive_metrics['avg_servers']:<12.1f} {reactive_metrics['overloaded_periods']:<15} {reactive_metrics['total_dropped_requests']:<12,.0f}\")\nprint(f\"{'Predictive':<15} {predictive_metrics['total_cost']:<12.2f} {predictive_metrics['avg_servers']:<12.1f} {predictive_metrics['overloaded_periods']:<15} {predictive_metrics['total_dropped_requests']:<12,.0f}\")\nprint(\"-\"*70)\nprint(f\"\\nKEY FINDINGS:\")\nprint(f\"  - Predictive scaling saves ${savings_predictive:.2f} vs fixed provisioning\")\nprint(f\"  - Cost reduction: {savings_predictive/fixed_peak_metrics.total_cost*100:.1f}%\")\nprint(\"=\"*70)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}