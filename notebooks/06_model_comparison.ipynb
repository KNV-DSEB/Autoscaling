{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 06: Model Comparison\n",
    "\n",
    "## Mục Tiêu\n",
    "- So sánh performance của các models: SARIMA, LightGBM, Prophet\n",
    "- Benchmark trên các granularities khác nhau\n",
    "- Chọn model tốt nhất cho autoscaling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Thêm src vào path\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from src.data.preprocessor import load_timeseries, split_train_test\n",
    "from src.features.feature_engineering import TimeSeriesFeatureEngineer\n",
    "from src.models.sarima import SARIMAForecaster\n",
    "from src.models.lightgbm_forecaster import LightGBMForecaster\n",
    "from src.models.prophet_forecaster import ProphetForecaster, PROPHET_AVAILABLE\n",
    "from src.models.evaluation import calculate_metrics, compare_models, print_metrics_table\n",
    "\n",
    "# Settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"Libraries loaded!\")\n",
    "print(f\"Prophet available: {PROPHET_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 15-minute data\n",
    "df = load_timeseries('../data/processed/timeseries_15min.parquet')\n",
    "df_clean = df[df['is_storm_period'] == 0].copy()\n",
    "\n",
    "# Train/Test split\n",
    "train, test = split_train_test(df_clean, test_start='1995-08-23')\n",
    "\n",
    "print(f\"Train: {len(train)} samples\")\n",
    "print(f\"Test: {len(test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for different models\n",
    "train_series = train['request_count']\n",
    "test_series = test['request_count']\n",
    "\n",
    "# For LightGBM - need features\n",
    "fe = TimeSeriesFeatureEngineer(df_clean)\n",
    "df_features = fe.create_all_features(target_col='request_count', granularity='15min')\n",
    "feature_cols = fe.get_feature_columns(df_features)\n",
    "X, y = fe.prepare_supervised(df_features, 'request_count', feature_cols, forecast_horizon=1)\n",
    "\n",
    "test_start = '1995-08-23'\n",
    "train_mask = X.index < test_start\n",
    "X_train, X_test = X[train_mask], X[~train_mask]\n",
    "y_train, y_test = y[train_mask], y[~train_mask]\n",
    "\n",
    "print(f\"\\nLightGBM features: {len(feature_cols)}\")\n",
    "print(f\"X_train: {X_train.shape}, X_test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store results\n",
    "model_results = {}\n",
    "model_predictions = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training SARIMA...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "sarima = SARIMAForecaster(\n",
    "    order=(2, 1, 2),\n",
    "    seasonal_order=(1, 1, 1, 96)\n",
    ")\n",
    "sarima.fit(train_series, verbose=True)\n",
    "\n",
    "# Predict\n",
    "sarima_preds = sarima.predict(steps=len(test_series))\n",
    "model_predictions['SARIMA'] = sarima_preds['forecast'].values\n",
    "\n",
    "# Calculate metrics\n",
    "model_results['SARIMA'] = calculate_metrics(\n",
    "    test_series.values[:len(sarima_preds)],\n",
    "    sarima_preds['forecast'].values\n",
    ")\n",
    "print(f\"\\nSARIMA RMSE: {model_results['SARIMA']['RMSE']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining LightGBM...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Validation split\n",
    "val_size = len(X_train) // 5\n",
    "X_val = X_train.iloc[-val_size:]\n",
    "y_val = y_train.iloc[-val_size:]\n",
    "X_train_lgb = X_train.iloc[:-val_size]\n",
    "y_train_lgb = y_train.iloc[:-val_size]\n",
    "\n",
    "lgbm = LightGBMForecaster(\n",
    "    n_estimators=1000,\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "lgbm.fit(X_train_lgb, y_train_lgb, X_val, y_val, verbose=100)\n",
    "\n",
    "# Predict\n",
    "lgbm_preds = lgbm.predict(X_test)\n",
    "model_predictions['LightGBM'] = lgbm_preds\n",
    "\n",
    "# Calculate metrics\n",
    "model_results['LightGBM'] = calculate_metrics(y_test.values, lgbm_preds)\n",
    "print(f\"\\nLightGBM RMSE: {model_results['LightGBM']['RMSE']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROPHET_AVAILABLE:\n",
    "    print(\"\\nTraining Prophet...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    prophet = ProphetForecaster(\n",
    "        seasonality_mode='multiplicative',\n",
    "        weekly_seasonality=True,\n",
    "        daily_seasonality=True,\n",
    "        add_hourly_seasonality=True\n",
    "    )\n",
    "    prophet.fit(train, target_col='request_count', verbose=True)\n",
    "    \n",
    "    # Predict\n",
    "    prophet_preds = prophet.predict(periods=len(test_series), freq='15min')\n",
    "    model_predictions['Prophet'] = prophet_preds['yhat'].values[:len(test_series)]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    model_results['Prophet'] = calculate_metrics(\n",
    "        test_series.values,\n",
    "        prophet_preds['yhat'].values[:len(test_series)]\n",
    "    )\n",
    "    print(f\"\\nProphet RMSE: {model_results['Prophet']['RMSE']:.4f}\")\n",
    "else:\n",
    "    print(\"Prophet not available, skipping...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison table\n",
    "print_metrics_table(model_results, \"Model Comparison - 15min Granularity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison DataFrame\n",
    "comparison_df = compare_models(model_results)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison of metrics\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "metrics_to_plot = ['MSE', 'RMSE', 'MAE', 'MAPE']\n",
    "colors = ['steelblue', 'coral', 'seagreen']\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    values = [model_results[m][metric] for m in model_results.keys()]\n",
    "    bars = axes[i].bar(model_results.keys(), values, color=colors[:len(values)])\n",
    "    axes[i].set_title(metric)\n",
    "    axes[i].set_ylabel('Value')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, values):\n",
    "        axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
    "                    f'{val:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.suptitle('Model Comparison - Metrics', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/model_comparison_metrics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all predictions vs actual\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "# Actual\n",
    "ax.plot(test_series.index[:len(model_predictions['SARIMA'])], \n",
    "        test_series.values[:len(model_predictions['SARIMA'])], \n",
    "        label='Actual', alpha=0.8, linewidth=1.5)\n",
    "\n",
    "# Each model\n",
    "for model_name, preds in model_predictions.items():\n",
    "    ax.plot(test_series.index[:len(preds)], preds, \n",
    "            label=model_name, alpha=0.7, linestyle='--')\n",
    "\n",
    "ax.set_xlabel('Timestamp')\n",
    "ax.set_ylabel('Request Count')\n",
    "ax.set_title('All Models: Predictions vs Actual')\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/model_comparison_predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom in on first day\n",
    "first_day = 96  # 96 intervals = 24 hours\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "ax.plot(test_series.index[:first_day], test_series.values[:first_day], \n",
    "        label='Actual', alpha=0.8, linewidth=2, color='black')\n",
    "\n",
    "colors = {'SARIMA': 'steelblue', 'LightGBM': 'coral', 'Prophet': 'seagreen'}\n",
    "for model_name, preds in model_predictions.items():\n",
    "    ax.plot(test_series.index[:first_day], preds[:first_day], \n",
    "            label=model_name, alpha=0.7, linestyle='--', \n",
    "            color=colors.get(model_name, 'gray'))\n",
    "\n",
    "ax.set_xlabel('Timestamp')\n",
    "ax.set_ylabel('Request Count')\n",
    "ax.set_title('First 24 Hours - Predictions vs Actual')\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/model_comparison_day1.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Error Analysis by Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create error DataFrame\n",
    "error_df = pd.DataFrame({\n",
    "    'timestamp': test_series.index[:len(model_predictions['SARIMA'])],\n",
    "    'actual': test_series.values[:len(model_predictions['SARIMA'])]\n",
    "})\n",
    "\n",
    "for model_name, preds in model_predictions.items():\n",
    "    error_df[f'{model_name}_pred'] = preds[:len(error_df)]\n",
    "    error_df[f'{model_name}_error'] = preds[:len(error_df)] - error_df['actual']\n",
    "    error_df[f'{model_name}_abs_error'] = np.abs(error_df[f'{model_name}_error'])\n",
    "\n",
    "error_df['hour'] = error_df['timestamp'].dt.hour\n",
    "error_df['day_of_week'] = error_df['timestamp'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error by hour\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "for model_name in model_predictions.keys():\n",
    "    hourly_error = error_df.groupby('hour')[f'{model_name}_abs_error'].mean()\n",
    "    ax.plot(hourly_error.index, hourly_error.values, marker='o', label=model_name)\n",
    "\n",
    "ax.set_xlabel('Hour of Day')\n",
    "ax.set_ylabel('Mean Absolute Error')\n",
    "ax.set_title('Prediction Error by Hour of Day')\n",
    "ax.set_xticks(range(24))\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/model_error_by_hour.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error by day of week\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "x = np.arange(7)\n",
    "width = 0.25\n",
    "\n",
    "for i, model_name in enumerate(model_predictions.keys()):\n",
    "    daily_error = error_df.groupby('day_of_week')[f'{model_name}_abs_error'].mean()\n",
    "    ax.bar(x + i*width, daily_error.values, width, label=model_name)\n",
    "\n",
    "ax.set_xlabel('Day of Week')\n",
    "ax.set_ylabel('Mean Absolute Error')\n",
    "ax.set_title('Prediction Error by Day of Week')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(days)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/model_error_by_day.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Selection for Autoscaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking by different criteria\n",
    "print(\"Model Rankings:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# By RMSE\n",
    "rmse_ranking = sorted(model_results.items(), key=lambda x: x[1]['RMSE'])\n",
    "print(\"\\nBy RMSE (lower is better):\")\n",
    "for i, (model, metrics) in enumerate(rmse_ranking):\n",
    "    print(f\"  {i+1}. {model}: {metrics['RMSE']:.4f}\")\n",
    "\n",
    "# By MAE\n",
    "mae_ranking = sorted(model_results.items(), key=lambda x: x[1]['MAE'])\n",
    "print(\"\\nBy MAE (lower is better):\")\n",
    "for i, (model, metrics) in enumerate(mae_ranking):\n",
    "    print(f\"  {i+1}. {model}: {metrics['MAE']:.4f}\")\n",
    "\n",
    "# By MAPE\n",
    "mape_ranking = sorted(model_results.items(), key=lambda x: x[1]['MAPE'])\n",
    "print(\"\\nBy MAPE (lower is better):\")\n",
    "for i, (model, metrics) in enumerate(mape_ranking):\n",
    "    print(f\"  {i+1}. {model}: {metrics['MAPE']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model for autoscaling\n",
    "best_model = rmse_ranking[0][0]\n",
    "best_metrics = rmse_ranking[0][1]\n",
    "\n",
    "print(f\"\\nRecommended Model for Autoscaling: {best_model}\")\n",
    "print(f\"RMSE: {best_metrics['RMSE']:.2f} requests/interval\")\n",
    "print(f\"MAE: {best_metrics['MAE']:.2f} requests/interval\")\n",
    "print(f\"MAPE: {best_metrics['MAPE']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all models\n",
    "sarima.save('../models/sarima_15min.pkl')\n",
    "lgbm.save('../models/lightgbm_15min.pkl')\n",
    "if PROPHET_AVAILABLE:\n",
    "    prophet.save('../models/prophet_15min.pkl')\n",
    "\n",
    "print(\"All models saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results\n",
    "comparison_df.to_csv('../reports/model_comparison_15min.csv', index=False)\n",
    "print(\"Comparison results saved to: ../reports/model_comparison_15min.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"                    MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nGranularity: 15 minutes\")\n",
    "print(f\"Test Period: {test.index.min()} to {test.index.max()}\")\n",
    "print(f\"Test Samples: {len(test)}\")\n",
    "print(f\"\\n\" + \"-\"*70)\n",
    "print(\"PERFORMANCE METRICS:\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Model':<15} {'RMSE':<12} {'MAE':<12} {'MAPE':<12} {'Rank':<6}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for i, (model, metrics) in enumerate(rmse_ranking):\n",
    "    print(f\"{model:<15} {metrics['RMSE']:<12.2f} {metrics['MAE']:<12.2f} {metrics['MAPE']:<12.2f}% {i+1:<6}\")\n",
    "\n",
    "print(\"-\"*70)\n",
    "print(f\"\\nBEST MODEL: {best_model}\")\n",
    "print(f\"  - Lowest RMSE: {best_metrics['RMSE']:.2f} requests/interval\")\n",
    "print(f\"  - Recommended for predictive autoscaling\")\n",
    "print(f\"\\nMODELS SAVED:\")\n",
    "print(f\"  - ../models/sarima_15min.pkl\")\n",
    "print(f\"  - ../models/lightgbm_15min.pkl\")\n",
    "if PROPHET_AVAILABLE:\n",
    "    print(f\"  - ../models/prophet_15min.pkl\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
