{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 06: Model Comparison\n",
    "\n",
    "## M·ª•c Ti√™u\n",
    "- So s√°nh performance c·ªßa c√°c models: SARIMA, LightGBM, Prophet\n",
    "- Benchmark tr√™n c√°c granularities kh√°c nhau\n",
    "- Ch·ªçn model t·ªët nh·∫•t cho autoscaling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Th√™m src v√†o path\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from src.data.preprocessor import load_timeseries, split_train_test\n",
    "from src.features.feature_engineering import TimeSeriesFeatureEngineer\n",
    "from src.models.sarima import SARIMAForecaster\n",
    "from src.models.lightgbm_forecaster import LightGBMForecaster\n",
    "from src.models.prophet_forecaster import ProphetForecaster, PROPHET_AVAILABLE\n",
    "from src.models.evaluation import calculate_metrics, compare_models, print_metrics_table\n",
    "\n",
    "# Settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"Libraries loaded!\")\n",
    "print(f\"Prophet available: {PROPHET_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 15-minute data\n",
    "df = load_timeseries('../data/processed/timeseries_15min.parquet')\n",
    "df_clean = df[df['is_storm_period'] == 0].copy()\n",
    "\n",
    "# Train/Test split\n",
    "train, test = split_train_test(df_clean, test_start='1995-08-23')\n",
    "\n",
    "print(f\"Train: {len(train)} samples\")\n",
    "print(f\"Test: {len(test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for different models\n",
    "train_series = train['request_count']\n",
    "test_series = test['request_count']\n",
    "\n",
    "# For LightGBM - need features\n",
    "fe = TimeSeriesFeatureEngineer(df_clean)\n",
    "df_features = fe.create_all_features(target_col='request_count', granularity='15min')\n",
    "feature_cols = fe.get_feature_columns(df_features)\n",
    "X, y = fe.prepare_supervised(df_features, 'request_count', feature_cols, forecast_horizon=1)\n",
    "\n",
    "test_start = '1995-08-23'\n",
    "train_mask = X.index < test_start\n",
    "X_train, X_test = X[train_mask], X[~train_mask]\n",
    "y_train, y_test = y[train_mask], y[~train_mask]\n",
    "\n",
    "print(f\"\\nLightGBM features: {len(feature_cols)}\")\n",
    "print(f\"X_train: {X_train.shape}, X_test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store results\n",
    "model_results = {}\n",
    "model_predictions = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training SARIMA...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "sarima = SARIMAForecaster(\n",
    "    order=(2, 1, 2),\n",
    "    seasonal_order=(1, 1, 1, 96)\n",
    ")\n",
    "sarima.fit(train_series, verbose=True)\n",
    "\n",
    "# Predict\n",
    "sarima_preds = sarima.predict(steps=len(test_series))\n",
    "model_predictions['SARIMA'] = sarima_preds['forecast'].values\n",
    "\n",
    "# Calculate metrics\n",
    "model_results['SARIMA'] = calculate_metrics(\n",
    "    test_series.values[:len(sarima_preds)],\n",
    "    sarima_preds['forecast'].values\n",
    ")\n",
    "print(f\"\\nSARIMA RMSE: {model_results['SARIMA']['RMSE']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining LightGBM...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Validation split\n",
    "val_size = len(X_train) // 5\n",
    "X_val = X_train.iloc[-val_size:]\n",
    "y_val = y_train.iloc[-val_size:]\n",
    "X_train_lgb = X_train.iloc[:-val_size]\n",
    "y_train_lgb = y_train.iloc[:-val_size]\n",
    "\n",
    "lgbm = LightGBMForecaster(\n",
    "    n_estimators=1000,\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "lgbm.fit(X_train_lgb, y_train_lgb, X_val, y_val, verbose=100)\n",
    "\n",
    "# Predict\n",
    "lgbm_preds = lgbm.predict(X_test)\n",
    "model_predictions['LightGBM'] = lgbm_preds\n",
    "\n",
    "# Calculate metrics\n",
    "model_results['LightGBM'] = calculate_metrics(y_test.values, lgbm_preds)\n",
    "print(f\"\\nLightGBM RMSE: {model_results['LightGBM']['RMSE']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROPHET_AVAILABLE:\n",
    "    print(\"\\nTraining Prophet...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    prophet = ProphetForecaster(\n",
    "        seasonality_mode='multiplicative',\n",
    "        weekly_seasonality=True,\n",
    "        daily_seasonality=True,\n",
    "        add_hourly_seasonality=True\n",
    "    )\n",
    "    prophet.fit(train, target_col='request_count', verbose=True)\n",
    "    \n",
    "    # Predict\n",
    "    prophet_preds = prophet.predict(periods=len(test_series), freq='15min')\n",
    "    model_predictions['Prophet'] = prophet_preds['yhat'].values[:len(test_series)]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    model_results['Prophet'] = calculate_metrics(\n",
    "        test_series.values,\n",
    "        prophet_preds['yhat'].values[:len(test_series)]\n",
    "    )\n",
    "    print(f\"\\nProphet RMSE: {model_results['Prophet']['RMSE']:.4f}\")\n",
    "else:\n",
    "    print(\"Prophet not available, skipping...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison table\n",
    "print_metrics_table(model_results, \"Model Comparison - 15min Granularity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison DataFrame\n",
    "comparison_df = compare_models(model_results)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison of metrics\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "metrics_to_plot = ['MSE', 'RMSE', 'MAE', 'MAPE']\n",
    "colors = ['steelblue', 'coral', 'seagreen']\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    values = [model_results[m][metric] for m in model_results.keys()]\n",
    "    bars = axes[i].bar(model_results.keys(), values, color=colors[:len(values)])\n",
    "    axes[i].set_title(metric)\n",
    "    axes[i].set_ylabel('Value')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, values):\n",
    "        axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
    "                    f'{val:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.suptitle('Model Comparison - Metrics', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/model_comparison_metrics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all predictions vs actual\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "# Actual\n",
    "ax.plot(test_series.index[:len(model_predictions['SARIMA'])], \n",
    "        test_series.values[:len(model_predictions['SARIMA'])], \n",
    "        label='Actual', alpha=0.8, linewidth=1.5)\n",
    "\n",
    "# Each model\n",
    "for model_name, preds in model_predictions.items():\n",
    "    ax.plot(test_series.index[:len(preds)], preds, \n",
    "            label=model_name, alpha=0.7, linestyle='--')\n",
    "\n",
    "ax.set_xlabel('Timestamp')\n",
    "ax.set_ylabel('Request Count')\n",
    "ax.set_title('All Models: Predictions vs Actual')\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/model_comparison_predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom in on first day\n",
    "first_day = 96  # 96 intervals = 24 hours\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "ax.plot(test_series.index[:first_day], test_series.values[:first_day], \n",
    "        label='Actual', alpha=0.8, linewidth=2, color='black')\n",
    "\n",
    "colors = {'SARIMA': 'steelblue', 'LightGBM': 'coral', 'Prophet': 'seagreen'}\n",
    "for model_name, preds in model_predictions.items():\n",
    "    ax.plot(test_series.index[:first_day], preds[:first_day], \n",
    "            label=model_name, alpha=0.7, linestyle='--', \n",
    "            color=colors.get(model_name, 'gray'))\n",
    "\n",
    "ax.set_xlabel('Timestamp')\n",
    "ax.set_ylabel('Request Count')\n",
    "ax.set_title('First 24 Hours - Predictions vs Actual')\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/model_comparison_day1.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Error Analysis by Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Find minimum prediction length across all models\nmin_len = min(len(preds) for preds in model_predictions.values())\n\n# Create error DataFrame with explicit lists to avoid index mismatch\nerror_df = pd.DataFrame({\n    'timestamp': list(test_series.index[:min_len]),\n    'actual': list(test_series.values[:min_len])\n})\n\nfor model_name, preds in model_predictions.items():\n    # Convert to list to avoid any index/array issues\n    pred_values = list(preds[:min_len]) if hasattr(preds, '__iter__') else [preds] * min_len\n    error_df[f'{model_name}_pred'] = pred_values\n    error_df[f'{model_name}_error'] = [p - a for p, a in zip(pred_values, error_df['actual'])]\n    error_df[f'{model_name}_abs_error'] = [abs(e) for e in error_df[f'{model_name}_error']]\n\nerror_df['hour'] = pd.to_datetime(error_df['timestamp']).dt.hour\nerror_df['day_of_week'] = pd.to_datetime(error_df['timestamp']).dt.dayofweek\n\nprint(f\"Error analysis using {min_len} samples (minimum across all models)\")\nprint(f\"Models included: {list(model_predictions.keys())}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error by hour\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "for model_name in model_predictions.keys():\n",
    "    hourly_error = error_df.groupby('hour')[f'{model_name}_abs_error'].mean()\n",
    "    ax.plot(hourly_error.index, hourly_error.values, marker='o', label=model_name)\n",
    "\n",
    "ax.set_xlabel('Hour of Day')\n",
    "ax.set_ylabel('Mean Absolute Error')\n",
    "ax.set_title('Prediction Error by Hour of Day')\n",
    "ax.set_xticks(range(24))\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/model_error_by_hour.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Error by day of week - handle partial data (test set may not have all 7 days)\nfig, ax = plt.subplots(figsize=(10, 5))\n\ndays_map = {0: 'Mon', 1: 'Tue', 2: 'Wed', 3: 'Thu', 4: 'Fri', 5: 'Sat', 6: 'Sun'}\navailable_days = sorted(error_df['day_of_week'].unique())\nday_labels = [days_map[d] for d in available_days]\nx = np.arange(len(available_days))\nwidth = 0.25\n\nfor i, model_name in enumerate(model_predictions.keys()):\n    daily_error = error_df.groupby('day_of_week')[f'{model_name}_abs_error'].mean()\n    # Only plot days that exist in data\n    values = [daily_error.get(d, 0) for d in available_days]\n    ax.bar(x + i*width, values, width, label=model_name)\n\nax.set_xlabel('Day of Week')\nax.set_ylabel('Mean Absolute Error')\nax.set_title('Prediction Error by Day of Week')\nax.set_xticks(x + width)\nax.set_xticklabels(day_labels)\nax.legend()\nplt.tight_layout()\nplt.savefig('../reports/figures/model_error_by_day.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Selection for Autoscaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking by different criteria\n",
    "print(\"Model Rankings:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# By RMSE\n",
    "rmse_ranking = sorted(model_results.items(), key=lambda x: x[1]['RMSE'])\n",
    "print(\"\\nBy RMSE (lower is better):\")\n",
    "for i, (model, metrics) in enumerate(rmse_ranking):\n",
    "    print(f\"  {i+1}. {model}: {metrics['RMSE']:.4f}\")\n",
    "\n",
    "# By MAE\n",
    "mae_ranking = sorted(model_results.items(), key=lambda x: x[1]['MAE'])\n",
    "print(\"\\nBy MAE (lower is better):\")\n",
    "for i, (model, metrics) in enumerate(mae_ranking):\n",
    "    print(f\"  {i+1}. {model}: {metrics['MAE']:.4f}\")\n",
    "\n",
    "# By MAPE\n",
    "mape_ranking = sorted(model_results.items(), key=lambda x: x[1]['MAPE'])\n",
    "print(\"\\nBy MAPE (lower is better):\")\n",
    "for i, (model, metrics) in enumerate(mape_ranking):\n",
    "    print(f\"  {i+1}. {model}: {metrics['MAPE']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model for autoscaling\n",
    "best_model = rmse_ranking[0][0]\n",
    "best_metrics = rmse_ranking[0][1]\n",
    "\n",
    "print(f\"\\nRecommended Model for Autoscaling: {best_model}\")\n",
    "print(f\"RMSE: {best_metrics['RMSE']:.2f} requests/interval\")\n",
    "print(f\"MAE: {best_metrics['MAE']:.2f} requests/interval\")\n",
    "print(f\"MAPE: {best_metrics['MAPE']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all models\n",
    "sarima.save('../models/sarima_15min.pkl')\n",
    "lgbm.save('../models/lightgbm_15min.pkl')\n",
    "if PROPHET_AVAILABLE:\n",
    "    prophet.save('../models/prophet_15min.pkl')\n",
    "\n",
    "print(\"All models saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results\n",
    "comparison_df.to_csv('../reports/model_comparison_15min.csv', index=False)\n",
    "print(\"Comparison results saved to: ../reports/model_comparison_15min.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"                    MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nGranularity: 15 minutes\")\n",
    "print(f\"Test Period: {test.index.min()} to {test.index.max()}\")\n",
    "print(f\"Test Samples: {len(test)}\")\n",
    "print(f\"\\n\" + \"-\"*70)\n",
    "print(\"PERFORMANCE METRICS:\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Model':<15} {'RMSE':<12} {'MAE':<12} {'MAPE':<12} {'Rank':<6}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for i, (model, metrics) in enumerate(rmse_ranking):\n",
    "    print(f\"{model:<15} {metrics['RMSE']:<12.2f} {metrics['MAE']:<12.2f} {metrics['MAPE']:<12.2f}% {i+1:<6}\")\n",
    "\n",
    "print(\"-\"*70)\n",
    "print(f\"\\nBEST MODEL: {best_model}\")\n",
    "print(f\"  - Lowest RMSE: {best_metrics['RMSE']:.2f} requests/interval\")\n",
    "print(f\"  - Recommended for predictive autoscaling\")\n",
    "print(f\"\\nMODELS SAVED:\")\n",
    "print(f\"  - ../models/sarima_15min.pkl\")\n",
    "print(f\"  - ../models/lightgbm_15min.pkl\")\n",
    "if PROPHET_AVAILABLE:\n",
    "    print(f\"  - ../models/prophet_15min.pkl\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 9. bytes_total Model Comparison\n\nNow we train all models on bytes_total (total bytes transferred) as a second target variable.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Prepare bytes_total data\ntrain_series_bytes = train['bytes_total']\ntest_series_bytes = test['bytes_total']\n\n# For LightGBM - need features for bytes_total\nfe_bytes = TimeSeriesFeatureEngineer(df_clean)\ndf_features_bytes = fe_bytes.create_all_features(target_col='bytes_total', granularity='15min')\nfeature_cols_bytes = fe_bytes.get_feature_columns(df_features_bytes)\nX_bytes, y_bytes = fe_bytes.prepare_supervised(df_features_bytes, 'bytes_total', feature_cols_bytes, forecast_horizon=1)\n\ntrain_mask_bytes = X_bytes.index < test_start\nX_train_bytes, X_test_bytes = X_bytes[train_mask_bytes], X_bytes[~train_mask_bytes]\ny_train_bytes, y_test_bytes = y_bytes[train_mask_bytes], y_bytes[~train_mask_bytes]\n\nprint(f\"bytes_total - Train: {len(train_series_bytes)}, Test: {len(test_series_bytes)}\")\nprint(f\"LightGBM features: {len(feature_cols_bytes)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Train all models on bytes_total\nbytes_results = {}\nbytes_predictions = {}\n\n# 1. SARIMA for bytes_total\nprint(\"Training SARIMA on bytes_total...\")\nprint(\"=\"*50)\nsarima_bytes = SARIMAForecaster(\n    order=(2, 1, 2),\n    seasonal_order=(1, 1, 0, 96)  # Using (1,1,0,96) to avoid memory issues\n)\nsarima_bytes.fit(train_series_bytes, verbose=True)\nsarima_bytes_preds = sarima_bytes.predict(steps=len(test_series_bytes))\nbytes_predictions['SARIMA'] = sarima_bytes_preds['forecast'].values\n\nbytes_results['SARIMA'] = calculate_metrics(\n    test_series_bytes.values[:len(sarima_bytes_preds)],\n    sarima_bytes_preds['forecast'].values\n)\nprint(f\"SARIMA bytes_total RMSE: {bytes_results['SARIMA']['RMSE']:.2f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 2. LightGBM for bytes_total\nprint(\"\\nTraining LightGBM on bytes_total...\")\nprint(\"=\"*50)\n\nval_size_bytes = len(X_train_bytes) // 5\nX_val_bytes = X_train_bytes.iloc[-val_size_bytes:]\ny_val_bytes = y_train_bytes.iloc[-val_size_bytes:]\nX_train_lgb_bytes = X_train_bytes.iloc[:-val_size_bytes]\ny_train_lgb_bytes = y_train_bytes.iloc[:-val_size_bytes]\n\nlgbm_bytes = LightGBMForecaster(\n    n_estimators=1000,\n    early_stopping_rounds=50\n)\nlgbm_bytes.fit(X_train_lgb_bytes, y_train_lgb_bytes, X_val_bytes, y_val_bytes, verbose=100)\n\nlgbm_bytes_preds = lgbm_bytes.predict(X_test_bytes)\nbytes_predictions['LightGBM'] = lgbm_bytes_preds\n\nbytes_results['LightGBM'] = calculate_metrics(y_test_bytes.values, lgbm_bytes_preds)\nprint(f\"LightGBM bytes_total RMSE: {bytes_results['LightGBM']['RMSE']:.2f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 3. Prophet for bytes_total\nif PROPHET_AVAILABLE:\n    print(\"\\nTraining Prophet on bytes_total...\")\n    print(\"=\"*50)\n    \n    prophet_bytes = ProphetForecaster(\n        seasonality_mode='multiplicative',\n        weekly_seasonality=True,\n        daily_seasonality=True,\n        add_hourly_seasonality=True\n    )\n    prophet_bytes.fit(train, target_col='bytes_total', verbose=True)\n    \n    prophet_bytes_preds = prophet_bytes.predict(periods=len(test_series_bytes), freq='15min')\n    bytes_predictions['Prophet'] = prophet_bytes_preds['yhat'].values[:len(test_series_bytes)]\n    \n    bytes_results['Prophet'] = calculate_metrics(\n        test_series_bytes.values,\n        prophet_bytes_preds['yhat'].values[:len(test_series_bytes)]\n    )\n    print(f\"Prophet bytes_total RMSE: {bytes_results['Prophet']['RMSE']:.2f}\")\nelse:\n    print(\"Prophet not available, skipping...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# bytes_total comparison table\nprint_metrics_table(bytes_results, \"bytes_total Model Comparison - 15min Granularity\")\n\n# Comparison DataFrame\nbytes_comparison_df = compare_models(bytes_results)\nbytes_comparison_df",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize bytes_total predictions\nfig, ax = plt.subplots(figsize=(16, 6))\n\nmin_len_bytes = min(len(preds) for preds in bytes_predictions.values())\nax.plot(test_series_bytes.index[:min_len_bytes], \n        test_series_bytes.values[:min_len_bytes], \n        label='Actual', alpha=0.8, linewidth=1.5)\n\nfor model_name, preds in bytes_predictions.items():\n    ax.plot(test_series_bytes.index[:min_len_bytes], preds[:min_len_bytes], \n            label=model_name, alpha=0.7, linestyle='--')\n\nax.set_xlabel('Timestamp')\nax.set_ylabel('Bytes Total')\nax.set_title('bytes_total: All Models Predictions vs Actual')\nax.legend()\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.savefig('../reports/figures/bytes_total_predictions.png', dpi=150, bbox_inches='tight')\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Complete Benchmark Table\n\nComprehensive comparison of all models across both target variables.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create comprehensive benchmark table\nall_results = {\n    'request_count': model_results,\n    'bytes_total': bytes_results\n}\n\nbenchmark_data = []\nfor target, results in all_results.items():\n    for model_name, metrics in results.items():\n        benchmark_data.append({\n            'Target': target,\n            'Model': model_name,\n            'MSE': metrics['MSE'],\n            'RMSE': metrics['RMSE'],\n            'MAE': metrics['MAE'],\n            'MAPE': metrics['MAPE']\n        })\n\nbenchmark_df = pd.DataFrame(benchmark_data)\nprint(\"=\"*80)\nprint(\"                    COMPLETE BENCHMARK TABLE\")\nprint(\"=\"*80)\nprint(benchmark_df.to_string(index=False))\nprint(\"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visual comparison for both targets\nfig, axes = plt.subplots(2, 3, figsize=(15, 8))\n\nmetrics_to_plot = ['RMSE', 'MAE', 'MAPE']\ntargets = ['request_count', 'bytes_total']\ncolors = {'SARIMA': 'steelblue', 'LightGBM': 'coral', 'Prophet': 'seagreen'}\n\nfor row, target in enumerate(targets):\n    for col, metric in enumerate(metrics_to_plot):\n        ax = axes[row, col]\n        results = all_results[target]\n        models = list(results.keys())\n        values = [results[m][metric] for m in models]\n        bars = ax.bar(models, values, color=[colors.get(m, 'gray') for m in models])\n        \n        ax.set_title(f'{target} - {metric}')\n        ax.set_ylabel(metric)\n        \n        # Add value labels\n        for bar, val in zip(bars, values):\n            label = f'{val:.2f}' if metric != 'MAPE' else f'{val:.1f}%'\n            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n                   label, ha='center', va='bottom', fontsize=9)\n\nplt.suptitle('Complete Model Comparison - Both Targets', fontsize=14)\nplt.tight_layout()\nplt.savefig('../reports/figures/complete_benchmark.png', dpi=150, bbox_inches='tight')\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Save all models for bytes_total\nsarima_bytes.save('../models/sarima_bytes_15min.pkl')\nlgbm_bytes.save('../models/lightgbm_bytes_15min.pkl')\nif PROPHET_AVAILABLE:\n    prophet_bytes.save('../models/prophet_bytes_15min.pkl')\n\nprint(\"All bytes_total models saved!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Save complete benchmark table\nbenchmark_df.to_csv('../reports/model_benchmark_complete.csv', index=False)\nprint(\"Complete benchmark saved to: ../reports/model_benchmark_complete.csv\")\n\n# Also save individual comparison CSVs\ncomparison_df.to_csv('../reports/model_comparison_request_count.csv', index=False)\nbytes_comparison_df.to_csv('../reports/model_comparison_bytes_total.csv', index=False)\nprint(\"Individual comparison CSVs saved!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 11. Final Summary",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"=\"*80)\nprint(\"                    FINAL MODEL COMPARISON SUMMARY\")\nprint(\"=\"*80)\n\nfor target in ['request_count', 'bytes_total']:\n    print(f\"\\n{'='*40}\")\n    print(f\"  Target: {target}\")\n    print(f\"{'='*40}\")\n    \n    results = all_results[target]\n    ranking = sorted(results.items(), key=lambda x: x[1]['RMSE'])\n    \n    print(f\"{'Model':<15} {'RMSE':<15} {'MAE':<15} {'MAPE':<15}\")\n    print(\"-\"*60)\n    for i, (model, metrics) in enumerate(ranking):\n        mape_str = f\"{metrics['MAPE']:.2f}%\"\n        rank = \"ü•á\" if i == 0 else (\"ü•à\" if i == 1 else \"ü•â\")\n        print(f\"{model:<15} {metrics['RMSE']:<15.2f} {metrics['MAE']:<15.2f} {mape_str:<15} {rank}\")\n    \n    best_model = ranking[0][0]\n    print(f\"\\n  ‚Üí Best model for {target}: {best_model}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"MODELS SAVED:\")\nprint(\"-\"*80)\nprint(\"request_count models:\")\nprint(\"  - models/sarima_15min.pkl\")\nprint(\"  - models/lightgbm_15min.pkl\")\nif PROPHET_AVAILABLE:\n    print(\"  - models/prophet_15min.pkl\")\nprint(\"\\nbytes_total models:\")\nprint(\"  - models/sarima_bytes_15min.pkl\")\nprint(\"  - models/lightgbm_bytes_15min.pkl\")\nif PROPHET_AVAILABLE:\n    print(\"  - models/prophet_bytes_15min.pkl\")\nprint(\"\\nBenchmark reports:\")\nprint(\"  - reports/model_benchmark_complete.csv\")\nprint(\"  - reports/model_comparison_request_count.csv\")\nprint(\"  - reports/model_comparison_bytes_total.csv\")\nprint(\"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}